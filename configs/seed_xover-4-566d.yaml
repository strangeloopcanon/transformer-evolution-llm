model:
  name: phi-tiny-evo-recur
  emb:
    dim: 512
    vocab: 50257
    rope: null
    dropout: 0.0
  blocks:
  - name: null
    attn:
      kind: GQA
      heads: 8
      head_dim: 64
      rope: yarn
      rope_theta: 13000.0
      sw: 128
      kv_groups: 2
      dropout: 0.0
      qk_norm_max: null
      sparsity: local_global
      block_size: null
      block_stride: null
      global_stride: 48
      dilation: null
    ffn:
      type: dense
      hidden: 2048
      activation: swiglu
      dropout: 0.0
    ssm: null
    extras:
    - type: retro
      memory_tokens: 256
      stride: 32
      aggregator: gate
      gating_weight: 0.25
  - name: null
    attn:
      kind: GQA
      heads: 8
      head_dim: 64
      rope: yarn
      rope_theta: null
      sw: 128
      kv_groups: 2
      dropout: 0.0
      qk_norm_max: null
      sparsity: local_block
      block_size: 64
      block_stride: 128
      global_stride: null
      dilation: null
    ffn:
      type: dense
      hidden: 2048
      activation: swiglu
      dropout: 0.0
    ssm:
      kind: mamba2
      d_state: 16
      d_conv: 4
      dt_rank: 8
      chunk: 128
      gate: 0.15
    extras: []
  - name: null
    attn:
      kind: GQA
      heads: 8
      head_dim: 64
      rope: null
      rope_theta: null
      sw: null
      kv_groups: null
      dropout: 0.0
      qk_norm_max: null
      sparsity: none
      block_size: null
      block_stride: null
      global_stride: null
      dilation: null
    ffn:
      type: dense
      hidden: 2048
      activation: swiglu
      dropout: 0.0
    ssm: null
    extras: []
  head:
    tie_embeddings: true
    vocab: 50257
  norm: rmsnorm
  recurrences:
  - start: 1
    end: 4
    adapter: gated
    adapter_dim: null
    concat_prelude: true
    init_state: zeros
    noise_std: 0.02
    train_recurrence: 1
    max_train_recurrence: 4
    curriculum_fraction: 0.3
    test_recurrences:
    - 1
    - 2
    - 4
    - 8
    - 16
train:
  lr: 0.0008
  warmup: 500
  clip: 1.0
  bf16: true
  grad_checkpoint: true
  max_tokens: 520000
  weight_decay: 0.02
  seed: 4242
  router_lb_coeff: 0.01
  router_entropy_coeff: 0.005
  optimizer:
    name: adamw
    lr: null
    betas: null
    eps: null
    weight_decay: null
data:
  tokenizer: gpt2
  hf_revision: main
  seq_len: 512
  batch_size: 1
  workers: 0
  shards:
  - name: codeparrot/github-code-clean
    split: train
    weight: 0.4
    cache_path: null
    revision: null
  - name: wikitext
    split: wikitext-2-raw-v1
    weight: 0.35
    cache_path: null
    revision: null
  - name: ag_news
    split: train
    weight: 0.25
    cache_path: null
    revision: null
  healing_shards:
  - name: ag_news
    split: train
    weight: 1.0
    cache_path: null
    revision: null
  healing_tokens: 200000
evolution:
  rung0_thresholds:
    gate_entropy_min: 0.8
    gate_entropy_max: 3.5
  rung1_tokens: 180000
  rung2_tokens: 520000
  population: 16
  topk_keep: 0.5
  crossover_prob: 0.4
  parent_selection: lexicase
  pareto_objectives:
  - ppl_code
  - ppl_math
  - long_recall
  - throughput
  - ram
  - layers
  - moe_blocks
  - novelty
priors:
  tokens_per_param: 4.0
  window_scale: 6.0
  rope_theta_default: 10000.0
  prior_weight: 0.0
  compute_penalty_weight: 0.0
