model:
  name: phi-tiny-evo-live
  emb:
    dim: 512
    vocab: 50257
  blocks:
    - attn:
        kind: GQA
        heads: 8
        head_dim: 64
        rope: yarn
        sw: 4096
      ffn:
        type: dense
        hidden: 2048
        activation: swiglu
      extras:
        - type: retro
          memory_tokens: 256
          stride: 32
          aggregator: gate
          gating_weight: 0.25
    - attn:
        kind: GQA
        heads: 8
        head_dim: 64
      ffn:
        type: moe
        hidden: 2048
        n_experts: 16
        k: 2
        capacity_factor: 1.2
        balance: 0.05
        shared: 1
      extras:
        - type: custom
          name: exp-gater
          params:
            dim: 256
    - attn:
        kind: GQA
        heads: 8
        head_dim: 64
      ffn:
        type: dense
        hidden: 2048
        activation: swiglu
      ssm:
        kind: mamba2
        d_state: 16
        d_conv: 4
        dt_rank: 8
        chunk: 128
        gate: 0.15
    - attn:
        kind: GQA
        heads: 8
        head_dim: 64
        rope: yarn
      ffn:
        type: moe
        hidden: 2048
        n_experts: 16
        k: 2
        capacity_factor: 1.2
        balance: 0.05
        shared: 1
      extras:
        - type: gated
          targets: ["attn", "ffn", "ssm"]
          init_weight: 0.2
          learnable: true
  head:
    tie_embeddings: true
    vocab: 50257
train:
  lr: 0.0008
  warmup: 400
  clip: 1.0
  bf16: true
  grad_ckpt: true
  max_tokens: 450000
  weight_decay: 0.02
  seed: 314
data:
  tokenizer: gpt2
  hf_revision: main
  seq_len: 256
  batch_size: 1
  workers: 0
  shards:
    - name: ag_news
      split: train
      weight: 0.3
    - name: wikitext
      split: wikitext-2-raw-v1
      weight: 0.4
    - name: codeparrot/github-code-clean
      split: train
      weight: 0.3
evolution:
  rung0_thresholds:
    gate_entropy_min: 0.8
    gate_entropy_max: 3.5
  rung1_tokens: 160000
  rung2_tokens: 520000
  population: 18
  topk_keep: 0.55
  crossover_prob: 0.5
  pareto_objectives:
    - ppl_code
    - ppl_math
    - long_recall
    - throughput
    - ram
