model:
  name: phi-tiny-moe-baseline
  emb:
    dim: 2048
    vocab: 100352
    rope: yarn
  blocks:
    - attn:
        kind: GQA
        heads: 16
        head_dim: 128
        rope: yarn
        sw: 2048
      ffn:
        type: moe
        hidden: 8192
        n_experts: 32
        k: 2
        balance: 0.05
        shared: 1
        capacity_factor: 1.3
      extras:
        - type: retro
          memory_tokens: 1024
          stride: 64
          aggregator: gate
          gating_weight: 0.4
        - type: custom
          name: exp-latent
          params:
            depth: 2
            mix: 0.25
    - attn:
        kind: GQA
        heads: 16
        head_dim: 128
      ffn:
        type: dense
        hidden: 8192
        activation: swiglu
      ssm:
        kind: mamba2
        d_state: 16
        d_conv: 4
        dt_rank: 8
        chunk: 1024
        gate: 0.1
    - attn:
        kind: GQA
        heads: 16
        head_dim: 128
      ffn:
        type: dense
        hidden: 8192
        activation: swiglu
  head:
    tie_embeddings: true
    vocab: 100352
train:
  lr: 0.0012
  warmup: 2000
  clip: 1.0
  bf16: true
  grad_ckpt: true
  max_tokens: 2000000
  weight_decay: 0.01
  seed: 42
data:
  tokenizer: microsoft/Phi-tiny-MoE-instruct
  hf_revision: main
  seq_len: 2048
  batch_size: 1
  workers: 0
  shards:
    - name: lighteval/mgsm
      split: train
      weight: 0.3
    - name: openwebtext
      split: train
      weight: 0.4
    - name: gsm8k
      split: train
      weight: 0.3
evolution:
  rung1_tokens: 200000
  rung2_tokens: 1000000
  population: 12
  topk_keep: 0.33
  crossover_prob: 0.25
  pareto_objectives:
    - ppl_code
    - ppl_math
    - long_recall
    - throughput
    - ram
